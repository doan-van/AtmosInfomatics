import numpy as np
import xarray as xr
import pandas as pd


era5 = xr.open_dataset('/Users/doan/Downloads/same_with_NAROdata_domain (1).nc')['t2m']


de = era5.groupby('valid_time.date').mean() - 273.15


ds = xr.open_dataset('/Users/doan/Downloads/NARO_data2023 (1).nc')['TMP_mea']


dna = ds.sel(latitude=slice(34.6, 35.6), longitude=slice(136.3, 137.3))
dna[0].plot()


latslide = (35, 35.2)
lonslide  = (136.55, 136.95)
dna = ds.sel(latitude=slice(latslide[0], latslide[1]), longitude=slice(lonslide[0], lonslide[1]))
print(dna)
dna[0].plot()


dex = de.sel(latitude=slice(latslide[1], latslide[0]), longitude=slice(lonslide[0], lonslide[1]))


dex[0]


# Load the low and high-resolution datasets
lo_ds = de
hi_ds = dna
# Perform the interpolation to match the high-resolution coordinates
# Assuming 'latitude' and 'longitude' as the coordinate names in both datasets
lo_regridded = lo_ds.interp(latitude=hi_ds.latitude, longitude=hi_ds.longitude, method="nearest")




lo_regridded[0].plot()


hi_ds[0].plot()





do = xr.Dataset( {'hi': ( ('date', 'latitude', 'longitude'), hi_ds.values), 
                  'lo': (('date', 'latitude', 'longitude'), lo_regridded.values) } )


do.coords['date'] = ('date', hi_ds.time.values) 
do.coords['latitude'] = ('latitude', hi_ds.latitude.values) 
do.coords['longitude'] = ('longitude', hi_ds.longitude.values) 


do['mask'] = (("latitude", "longitude"), do.hi.isel(date=0).isnull().values)


do['hi'] = do.hi.fillna(do.lo)



do['hi'][0].plot()


do['mask'].plot()


do.to_netcdf('data/nagoya_input.nc')








dinput = xr.open_dataset('data/nagoya_input.nc')


dinput.lo[0].plot()
#n = 365
n = dinput['hi'].shape[0]
print('we have ', n, 'sample')


# separate data to three part: 1) training; 2) validation; 3) test
ii = np.arange(n)
n_train = int(round(.7*n))
print('extract: ', n_train, 'for training')


import random
itrain=[]
while len(itrain) !=  n_train :
   r=random.randint(0,n-1)
   if r not in itrain: itrain.append(r)


print( 'check len of training data: ', len(np.unique(itrain)), len(itrain))
print('get 70% of whole data to train: ', len(itrain))


i2 = np.setdiff1d(ii, itrain)
print('then we have remaining ', len(i2))


n_vt = len(i2)
n_val = int(round(.6*n_vt))


j1=[]
while len(j1) !=  n_val :
    r=random.randint(0, n_vt - 1)
    if r not in j1: j1.append(r)


ival = i2[j1]
itest = np.setdiff1d(i2, ival)


print(np.unique(np.union1d(itrain, np.union1d(ival, itest))).shape)
print(len(itrain) + len(ival) + len(itest))
# -





dtrain0 = dinput.isel(time=itrain)
dval0 = dinput.isel(time=ival)
dtest0 = dinput.isel(time=itest)


# =====
dtrain0.to_netcdf('data/nagoya_train.nc')
dval0.to_netcdf('data/nagoya_val.nc')
dtest0.to_netcdf('data/nagoya_test.nc')





